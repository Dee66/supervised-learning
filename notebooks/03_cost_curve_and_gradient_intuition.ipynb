{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "4ba9e9e6",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "# Cost Curves & Gradient Intuition — Why the Slope Drives Learning\n",
                "\n",
                "Welcome back! In Notebook 1, we set the stage for supervised learning — what it is, and why it matters. Now, let's get practical: how do models actually learn, and why should you care about the mechanics under the hood?\n",
                "\n",
                "This notebook is about **optimization** — the process that helps a model get better, step by step. But this isn't just for ML engineers: if you ever wonder why your model sometimes gets stuck, or why it suddenly makes wild predictions, the answer is often hidden in these curves and slopes.\n",
                "\n",
                "---\n",
                "\n",
                "**Visual Roadmap:**\n",
                "- 📈 **Move a slider:** See how changing a model parameter changes the cost — like tuning a knob and watching your system's performance respond. This is what happens every time your model updates itself.\n",
                "- ➡️ **Explore the tangent (gradient):** The slope at any point is the model's \"sense of direction\" — it tells the system which way to move to get better, and how big a step to take.\n",
                "- 🟢 **Animate gradient descent:** Watch the model \"learn\" in real time. You'll see why sometimes it races ahead, sometimes it stalls, and sometimes it spins out of control — all depending on how you set the learning rate.\n",
                "\n",
                "*Why does this matter for you? Because every spike, stall, or wild jump you see here has a direct parallel in real-world ML systems. If you understand these patterns, you can spot trouble early, tune your models with confidence, and avoid costly surprises in production.*\n",
                "\n",
                "---\n",
                "\n",
                "> **Architect’s Note:**  \n",
                "In production, instability often comes from not understanding how optimization works at the parameter level. Monitoring cost curves and gradient magnitudes isn’t just for data scientists — it’s essential for anyone who wants reliable, scalable, and safe ML systems.\n",
                "\n",
                "---\n",
                "\n",
                "**Notebook Series Context:**\n",
                "This is Notebook 2 of 8. Each notebook builds on the last, layering intuition and practical insight. By the end, you’ll have a real-world view of how ML systems learn, adapt, and sometimes fail — and how to design for stability, not just accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "0110c9e6",
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Imports and cost function setup for gradient visualization\n",
                "\n",
                "import numpy as np\n",
                "import plotly.graph_objects as go\n",
                "import ipywidgets as widgets\n",
                "from IPython.display import display, Markdown\n",
                "\n",
                "# Define a simple quadratic cost function: J(w) = (w - 2)^2 + 1\n",
                "def cost_fn(w):\n",
                "    return (w - 2) ** 2 + 1\n",
                "\n",
                "# Its derivative (gradient): dJ/dw = 2(w - 2)\n",
                "def grad_fn(w):\n",
                "    return 2 * (w - 2)\n",
                "\n",
                "# Range of parameter values for visualization\n",
                "w_range = np.linspace(-2, 6, 200)\n",
                "cost_vals = cost_fn(w_range)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "e29d2c5f",
            "metadata": {
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "360f06efa387450b80be256c7877bafc",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "interactive(children=(FloatSlider(value=0.0, description='w (parameter):', layout=Layout(width='60%'), max=6.0…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "<function __main__.plot_cost_and_tangent(w0)>"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Interactive cost curve with tangent (gradient) at a chosen point\n",
                "\n",
                "w_slider = widgets.FloatSlider(\n",
                "    value=0.0, min=-2, max=6, step=0.05,\n",
                "    description=\"w (parameter):\", continuous_update=True, readout_format=\".2f\", style={'description_width': '120px'}, layout=widgets.Layout(width='60%')\n",
                ")\n",
                "\n",
                "def plot_cost_and_tangent(w0):\n",
                "    fig = go.Figure()\n",
                "\n",
                "    # Plot the cost curve (only once, as a single trace)\n",
                "    fig.add_trace(go.Scatter(\n",
                "        x=w_range, y=cost_vals, mode='lines', name='Cost Curve J(w)',\n",
                "        line=dict(color='#1f77b4', width=3)\n",
                "    ))\n",
                "\n",
                "    # Point on the curve\n",
                "    y0 = cost_fn(w0)\n",
                "    fig.add_trace(go.Scatter(\n",
                "        x=[w0], y=[y0], mode='markers', name='Current w',\n",
                "        marker=dict(color='#d62728', size=12, symbol='circle')\n",
                "    ))\n",
                "\n",
                "    # Tangent line at w0\n",
                "    grad = grad_fn(w0)\n",
                "    tangent_x = np.array([w0 - 1, w0 + 1])\n",
                "    tangent_y = cost_fn(w0) + grad * (tangent_x - w0)\n",
                "    fig.add_trace(go.Scatter(\n",
                "        x=tangent_x, y=tangent_y, mode='lines', name='Tangent (Gradient)',\n",
                "        line=dict(color='#ff7f0e', dash='dash', width=2)\n",
                "    ))\n",
                "\n",
                "    # Gradient arrow\n",
                "    arrow_scale = 0.7\n",
                "    fig.add_annotation(\n",
                "        x=w0 + arrow_scale * np.sign(grad),\n",
                "        y=y0 + grad * arrow_scale * np.sign(grad),\n",
                "        ax=w0, ay=y0,\n",
                "        xref='x', yref='y', axref='x', ayref='y',\n",
                "        showarrow=True, arrowhead=3, arrowsize=1.2, arrowwidth=2, arrowcolor='#ff7f0e',\n",
                "        opacity=0.8\n",
                "    )\n",
                "\n",
                "    fig.update_layout(\n",
                "        title=\"Cost Curve with Tangent — The Slope is the Update Direction\",\n",
                "        xaxis_title=\"Parameter w\",\n",
                "        yaxis_title=\"Cost J(w)\",\n",
                "        width=800, height=450,\n",
                "        plot_bgcolor=\"#f8f8fa\",\n",
                "        margin=dict(l=30, r=30, t=60, b=30),\n",
                "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
                "    )\n",
                "\n",
                "    fig.add_annotation(\n",
                "        x=w0, y=y0, text=f\"Gradient: {grad:.2f}\", showarrow=False,\n",
                "        font=dict(color=\"#ff7f0e\", size=14), yshift=30, xshift=0, bgcolor=\"#fff\"\n",
                "    )\n",
                "\n",
                "    # Only display the figure and markdown once per interaction\n",
                "    display(fig)\n",
                "    display(Markdown(\n",
                "        f\"**At w = {w0:.2f}:** The tangent’s slope (gradient) is <span style='color:#ff7f0e'><b>{grad:.2f}</b></span>. \"\n",
                "        \"This is the direction and speed of parameter updates in gradient descent.\"\n",
                "    ))\n",
                "\n",
                "# IMPORTANT: Only run this cell ONCE in the notebook.\n",
                "widgets.interact(plot_cost_and_tangent, w0=w_slider)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9a639001",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "---\n",
                "## Real-World Reflection: Why Gradient Intuition Matters in Production\n",
                "\n",
                "In real ML systems, the shape of the cost curve and the magnitude of its gradient directly impact:\n",
                "- **Stability:** Steep slopes can cause overshooting or divergence; flat regions can stall learning.\n",
                "- **Retraining schedules:** Plateaus or sharp valleys may require dynamic learning rates or early stopping.\n",
                "- **Operational risk:** Misunderstanding optimization dynamics leads to brittle deployments and costly failures.\n",
                "\n",
                "> **Architect’s Note:**  \n",
                "> Always monitor cost and gradient behavior in production. Unexpected spikes or plateaus are early signals of data drift, poor feature scaling, or infrastructure bottlenecks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "f1d31543",
            "metadata": {
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8a0c97d2d9aa4ef4bb812f9641fbf9e3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "interactive(children=(FloatSlider(value=5.5, description='Start w', max=6.0, min=-2.0), FloatSlider(value=0.2,…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "<function __main__.animate_gradient_descent(w_start=5.5, lr=0.2, steps=12)>"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Animate gradient descent: see how the parameter \"rolls downhill\" on the cost curve\n",
                "\n",
                "import time\n",
                "\n",
                "def animate_gradient_descent(w_start=5.5, lr=0.2, steps=12):\n",
                "    ws = [w_start]\n",
                "    for _ in range(steps):\n",
                "        grad = grad_fn(ws[-1])\n",
                "        ws.append(ws[-1] - lr * grad)\n",
                "    ws = np.array(ws)\n",
                "    ys = cost_fn(ws)\n",
                "\n",
                "    fig = go.Figure()\n",
                "\n",
                "    # Cost curve\n",
                "    fig.add_trace(go.Scatter(\n",
                "        x=w_range, y=cost_vals, mode='lines', name='Cost Curve J(w)',\n",
                "        line=dict(color='#1f77b4', width=3)\n",
                "    ))\n",
                "\n",
                "    # Path of gradient descent\n",
                "    fig.add_trace(go.Scatter(\n",
                "        x=ws, y=ys, mode='markers+lines', name='GD Path',\n",
                "        marker=dict(color='#2ca02c', size=10, symbol='circle'),\n",
                "        line=dict(color='#2ca02c', width=2, dash='dot')\n",
                "    ))\n",
                "\n",
                "    # Start and end points\n",
                "    fig.add_trace(go.Scatter(\n",
                "        x=[ws[0]], y=[ys[0]], mode='markers', name='Start',\n",
                "        marker=dict(color='#d62728', size=14, symbol='diamond')\n",
                "    ))\n",
                "    fig.add_trace(go.Scatter(\n",
                "        x=[ws[-1]], y=[ys[-1]], mode='markers', name='End',\n",
                "        marker=dict(color='#9467bd', size=14, symbol='star')\n",
                "    ))\n",
                "\n",
                "    fig.update_layout(\n",
                "        title=\"Gradient Descent Path on Cost Curve\",\n",
                "        xaxis_title=\"Parameter w\",\n",
                "        yaxis_title=\"Cost J(w)\",\n",
                "        width=800, height=450,\n",
                "        plot_bgcolor=\"#f8f8fa\",\n",
                "        margin=dict(l=30, r=30, t=60, b=30),\n",
                "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
                "    )\n",
                "\n",
                "    display(fig)\n",
                "    display(Markdown(\n",
                "        f\"**Gradient Descent:** Starting from <b>w = {w_start}</b>, each step moves opposite the gradient (slope), scaled by the learning rate (<b>lr = {lr}</b>).<br>\"\n",
                "        \"This is why the derivative matters — the slope is the update direction.\"\n",
                "    ))\n",
                "\n",
                "widgets.interact(\n",
                "    animate_gradient_descent,\n",
                "    w_start=widgets.FloatSlider(value=5.5, min=-2, max=6, step=0.1, description=\"Start w\"),\n",
                "    lr=widgets.FloatSlider(value=0.2, min=0.01, max=1.0, step=0.01, description=\"Learning Rate\"),\n",
                "    steps=widgets.IntSlider(value=12, min=3, max=30, step=1, description=\"Steps\")\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "536923d0",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "---\n",
                "### Architect’s Note: Why This Animation Matters\n",
                "\n",
                "Gradient descent is the backbone of most ML optimization. But for the average Jo (or anyone running a business), here's the real value:\n",
                "\n",
                "- **Convergence speed:** If your model learns too slowly, you're wasting time and money. If it learns too fast, it might miss the best answer or even break.\n",
                "- **System reliability:** Wild swings or stuck models can mean failed retraining jobs, bad predictions, or lost trust.\n",
                "- **Monitoring:** Spikes or plateaus in cost or gradient are like warning lights on your dashboard — they tell you when something's off, whether it's your data, your features, or your infrastructure.\n",
                "\n",
                "**Bottom line:** If you want your ML system to be stable, predictable and valuable, keep an eye on how it learns — not just what it predicts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "821e0d12",
            "metadata": {
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a742de4b9c734e90a0d5e32dd6df1aa4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "interactive(children=(FloatSlider(value=5.5, description='Start w', max=6.0, min=-2.0), IntSlider(value=12, de…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "<function __main__.compare_learning_rates(w_start=5.5, steps=12)>"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Visualizing failure cases: Overshooting and stalling with different learning rates\n",
                "\n",
                "def compare_learning_rates(w_start=5.5, steps=12):\n",
                "    lrs = [0.05, 0.2, 0.7, 1.1]  # Low, good, high, too high (diverges)\n",
                "    colors = ['#1f77b4', '#2ca02c', '#ff7f0e', '#d62728']\n",
                "    fig = go.Figure()\n",
                "\n",
                "    # Cost curve\n",
                "    fig.add_trace(go.Scatter(\n",
                "        x=w_range, y=cost_vals, mode='lines', name='Cost Curve J(w)',\n",
                "        line=dict(color='#888', width=2, dash='dot')\n",
                "    ))\n",
                "\n",
                "    for lr, color in zip(lrs, colors):\n",
                "        ws = [w_start]\n",
                "        diverged = False\n",
                "        for _ in range(steps):\n",
                "            grad = grad_fn(ws[-1])\n",
                "            next_w = ws[-1] - lr * grad\n",
                "            # Divergence check: if cost explodes, break\n",
                "            if abs(next_w) > 1e3 or abs(cost_fn(next_w)) > 1e5:\n",
                "                diverged = True\n",
                "                break\n",
                "            ws.append(next_w)\n",
                "        ws = np.array(ws)\n",
                "        ys = cost_fn(ws)\n",
                "        fig.add_trace(go.Scatter(\n",
                "            x=ws, y=ys, mode='markers+lines',\n",
                "            name=f\"lr={lr}{' (diverges)' if diverged else ''}\",\n",
                "            marker=dict(size=8, color=color),\n",
                "            line=dict(width=2, color=color, dash='solid' if not diverged else 'dash')\n",
                "        ))\n",
                "\n",
                "    fig.update_layout(\n",
                "        title=\"Gradient Descent Paths for Different Learning Rates\",\n",
                "        xaxis_title=\"Parameter w\",\n",
                "        yaxis_title=\"Cost J(w)\",\n",
                "        width=800, height=450,\n",
                "        plot_bgcolor=\"#f8f8fa\",\n",
                "        margin=dict(l=30, r=30, t=60, b=30),\n",
                "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
                "    )\n",
                "\n",
                "    display(fig)\n",
                "    display(Markdown(\n",
                "        \"**Failure Cases:**\\n\"\n",
                "        \"- Low learning rate: slow progress, may stall.\\n\"\n",
                "        \"- Good learning rate: fast, stable convergence.\\n\"\n",
                "        \"- High learning rate: overshoots, may oscillate.\\n\"\n",
                "        \"- Too high: diverges (cost explodes).<br><br>\"\n",
                "        \"**Architect’s Note:** In production, tuning learning rates is critical for stability. Always monitor for divergence or stalling during training.\"\n",
                "    ))\n",
                "\n",
                "widgets.interact(compare_learning_rates, w_start=widgets.FloatSlider(value=5.5, min=-2, max=6, step=0.1, description=\"Start w\"), steps=widgets.IntSlider(value=12, min=5, max=30, step=1, description=\"Steps\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "10775adf",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "---\n",
                "### Architect’s Note: Learning Rate Tuning & System Stability\n",
                "\n",
                "The right learning rate isn't just a technical detail — it's the difference between a model that learns, a model that stalls, and a model that crashes.\n",
                "\n",
                "- **Too low:** Training drags on, wasting time and money. Your model might never get good enough.\n",
                "- **Too high:** The model can \"blow up\" — making wild guesses, failing to converge, or even breaking your pipeline.\n",
                "- **Just right:** You get fast, stable learning and a model you can trust.\n",
                "\n",
                "**In practice:**  \n",
                "Keep an eye on your cost and gradient curves during training. Set up alerts for when things go off track. Even with fancy optimizers, always check your results on real data — and remember, a stable system is a valuable system."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d0efd0b1",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "---\n",
                "## Summary & Next Steps\n",
                "\n",
                "- **You’ve seen:** How cost curves, gradients, and learning rates interact to drive model optimization — and how their mismanagement leads to instability in real systems.\n",
                "- **Architect’s takeaway:** Always visualize and monitor these dynamics in production. Proactive detection of instability is key to robust, scalable ML.\n",
                "\n",
                "**Up next:**  \n",
                "We’ll extend these concepts to more complex cost landscapes (e.g., elliptical contours), and show how optimization behaves in higher dimensions — a critical leap for real-world ML architecture.\n",
                "\n",
                "*Continue building modularly. Each notebook should deepen your intuition and operational confidence.*"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0ac38a65",
            "metadata": {},
            "source": [
                "---\n",
                "**Previous:** [Notebook 2 – Supervised Learning Demystified: From First Principles to Production Readiness](02_supervised_learning_systems.ipynb)  \n",
                "**Next:** [Notebook 4 – Cost Curve & Gradient Intuition, Part 2](04_cost_curve_and_gradient_intuition_part_2.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
