{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b166fa",
   "metadata": {},
   "source": [
    "# Cost Curves & Gradient Intuition ‚Äî From Classroom to Production\n",
    "\n",
    "Most ML tutorials stop at ‚Äúhere‚Äôs a curve, here‚Äôs a slope.‚Äù But in real systems, these curves *drive* everything: how fast your model learns, how stable it is, and how much risk you‚Äôre taking on every time you retrain.\n",
    "\n",
    "This notebook connects the math to the messiness of production. You‚Äôll see not just how models ‚Äúroll downhill,‚Äù but why the shape and slope of the cost curve are the difference between a model that quietly improves and one that blows up your pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c21418",
   "metadata": {},
   "source": [
    "**Visual Roadmap:**\n",
    "- üìà **Cost curve:** See how model error changes as you tweak a parameter.\n",
    "- ‚û°Ô∏è **Tangent (gradient):** The slope is the model‚Äôs ‚Äústeering wheel.‚Äù It tells your optimizer which way to turn, and how hard.\n",
    "- üü¢ **Gradient descent animation:** Watch the model ‚Äúfeel‚Äù its way downhill. This is what‚Äôs happening every time your model updates ‚Äî for better or worse.\n",
    "\n",
    "*Why care? Because every spike, stall, or wild jump you see here has a direct parallel in real-world ML systems. If you understand these patterns, you can spot trouble early, tune your models with confidence, and avoid costly surprises in production.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eaecd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and cost function setup for gradient visualization\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# In production, cost functions are rarely this simple ‚Äî but the principles are the same.\n",
    "def cost_fn(w):\n",
    "    return (w - 2) ** 2 + 1\n",
    "\n",
    "def grad_fn(w):\n",
    "    return 2 * (w - 2)\n",
    "\n",
    "w_range = np.linspace(-2, 6, 200)\n",
    "cost_vals = cost_fn(w_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0024a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2347f408ce4a7dbcabb4ed5f077d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w (parameter):', layout=Layout(width='60%'), max=6.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_cost_and_tangent(w0)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interactive cost curve with tangent (gradient) at a chosen point\n",
    "\n",
    "w_slider = widgets.FloatSlider(\n",
    "    value=0.0, min=-2, max=6, step=0.05,\n",
    "    description=\"w (parameter):\", continuous_update=True, readout_format=\".2f\", style={'description_width': '120px'}, layout=widgets.Layout(width='60%')\n",
    ")\n",
    "\n",
    "def plot_cost_and_tangent(w0):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=w_range, y=cost_vals, mode='lines', name='Cost Curve J(w)',\n",
    "        line=dict(color='#1f77b4', width=3)\n",
    "    ))\n",
    "\n",
    "    y0 = cost_fn(w0)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[w0], y=[y0], mode='markers', name='Current w',\n",
    "        marker=dict(color='#d62728', size=12, symbol='circle')\n",
    "    ))\n",
    "\n",
    "    grad = grad_fn(w0)\n",
    "    tangent_x = np.array([w0 - 1, w0 + 1])\n",
    "    tangent_y = cost_fn(w0) + grad * (tangent_x - w0)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=tangent_x, y=tangent_y, mode='lines', name='Tangent (Gradient)',\n",
    "        line=dict(color='#ff7f0e', dash='dash', width=2)\n",
    "    ))\n",
    "\n",
    "    arrow_scale = 0.7\n",
    "    fig.add_annotation(\n",
    "        x=w0 + arrow_scale * np.sign(grad),\n",
    "        y=y0 + grad * arrow_scale * np.sign(grad),\n",
    "        ax=w0, ay=y0,\n",
    "        xref='x', yref='y', axref='x', ayref='y',\n",
    "        showarrow=True, arrowhead=3, arrowsize=1.2, arrowwidth=2, arrowcolor='#ff7f0e',\n",
    "        opacity=0.8\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Cost Curve with Tangent ‚Äî The Slope is the Update Direction\",\n",
    "        xaxis_title=\"Parameter w\",\n",
    "        yaxis_title=\"Cost J(w)\",\n",
    "        width=800, height=450,\n",
    "        plot_bgcolor=\"#f8f8fa\",\n",
    "        margin=dict(l=30, r=30, t=60, b=30),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "    )\n",
    "\n",
    "    fig.add_annotation(\n",
    "        x=w0, y=y0, text=f\"Gradient: {grad:.2f}\", showarrow=False,\n",
    "        font=dict(color=\"#ff7f0e\", size=14), yshift=30, xshift=0, bgcolor=\"#fff\"\n",
    "    )\n",
    "\n",
    "    display(fig)\n",
    "    display(Markdown(\n",
    "        f\"**At w = {w0:.2f}:** The tangent‚Äôs slope (gradient) is <span style='color:#ff7f0e'><b>{grad:.2f}</b></span>.<br>\"\n",
    "        \"This is the direction and speed of parameter updates in gradient descent.<br><br>\"\n",
    "        \"**Why does this matter in production?** Because if your model misreads the slope, it can stall, overshoot, or even diverge ‚Äî leading to instability, wasted compute, or business risk. \"\n",
    "        \"Understanding this helps you design safer, more reliable ML systems.\"\n",
    "    ))\n",
    "\n",
    "widgets.interact(plot_cost_and_tangent, w0=w_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8544241c",
   "metadata": {},
   "source": [
    "> **Architect‚Äôs Note:**  \n",
    "> In production, model instability is rarely just a math bug ‚Äî it‚Äôs often a sign that the optimizer is ‚Äúfighting‚Äù the cost curve. If you know how the slope (gradient) drives updates, you can spot and fix issues before they hit your users or your bottom line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f15b4a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78334b0bf6a40e8985f1568828961a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=5.5, description='Start w', max=6.0, min=-2.0), FloatSlider(value=0.2,‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.animate_gradient_descent(w_start=5.5, lr=0.2, steps=12)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Animate gradient descent: see how the parameter ‚Äúrolls downhill‚Äù on the cost curve\n",
    "\n",
    "def animate_gradient_descent(w_start=5.5, lr=0.2, steps=12):\n",
    "    ws = [w_start]\n",
    "    for _ in range(steps):\n",
    "        grad = grad_fn(ws[-1])\n",
    "        ws.append(ws[-1] - lr * grad)\n",
    "    ws = np.array(ws)\n",
    "    ys = cost_fn(ws)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=w_range, y=cost_vals, mode='lines', name='Cost Curve J(w)',\n",
    "        line=dict(color='#1f77b4', width=3)\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=ws, y=ys, mode='markers+lines', name='GD Path',\n",
    "        marker=dict(color='#2ca02c', size=10, symbol='circle'),\n",
    "        line=dict(color='#2ca02c', width=2, dash='dot')\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[ws[0]], y=[ys[0]], mode='markers', name='Start',\n",
    "        marker=dict(color='#d62728', size=14, symbol='diamond')\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[ws[-1]], y=[ys[-1]], mode='markers', name='End',\n",
    "        marker=dict(color='#9467bd', size=14, symbol='star')\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Gradient Descent Path on Cost Curve\",\n",
    "        xaxis_title=\"Parameter w\",\n",
    "        yaxis_title=\"Cost J(w)\",\n",
    "        width=800, height=450,\n",
    "        plot_bgcolor=\"#f8f8fa\",\n",
    "        margin=dict(l=30, r=30, t=60, b=30),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "    )\n",
    "\n",
    "    display(fig)\n",
    "    display(Markdown(\n",
    "        f\"**Gradient Descent:** Starting from <b>w = {w_start}</b>, each step moves opposite the gradient (slope), scaled by the learning rate (<b>lr = {lr}</b>).<br>\"\n",
    "        \"This is why the derivative matters ‚Äî the slope is the update direction.<br><br>\"\n",
    "        \"**Why does this matter for your team?** Because if the steps are too big or too small, your model can get stuck, bounce around, or even fail to learn at all. Watching this process helps you tune your system for stability and speed ‚Äî and avoid costly retraining cycles.\"\n",
    "    ))\n",
    "\n",
    "widgets.interact(\n",
    "    animate_gradient_descent,\n",
    "    w_start=widgets.FloatSlider(value=5.5, min=-2, max=6, step=0.1, description=\"Start w\"),\n",
    "    lr=widgets.FloatSlider(value=0.2, min=0.01, max=1.0, step=0.01, description=\"Learning Rate\"),\n",
    "    steps=widgets.IntSlider(value=12, min=3, max=30, step=1, description=\"Steps\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ef474",
   "metadata": {},
   "source": [
    "---\n",
    "## Real-World Reflection: Why Gradient Intuition Matters for Teams\n",
    "\n",
    "- **Convergence speed:** If your model learns too slowly, you‚Äôre wasting time and compute. Too fast, and you risk instability or missing the best solution.\n",
    "- **System reliability:** Wild swings or stuck models can mean failed retraining jobs, bad predictions, or lost trust ‚Äî all of which impact business outcomes.\n",
    "- **Monitoring:** Spikes or plateaus in cost or gradient are like warning lights on your dashboard ‚Äî they tell you when something‚Äôs off, whether it‚Äôs your data, your features, or your infrastructure.\n",
    "\n",
    "**Bottom line:** If you want your ML system to be stable, predictable, and valuable, keep an eye on how it learns ‚Äî not just what it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff2c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
